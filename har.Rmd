---
title: "Machine Learning Analysis of Human Activity Recognition"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Introduction

In this report, we analyze the physical activity of some individuals. In particular, we want to predict how well they did the activity. To do so, we have a training and a testing sets with data on individuals performing unilateral dumbbell biceps curl (see http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har). In the case of the training set, we also know how well they did the activity. This information is encoded in the variable "classe", being "A" the case in which they performed the exercise correctly and all the other values are such that they made some mistake: "B" throwing the elbows to the front, "C" lifting the dumbbell only halfway, "D" lowering the dumbbell only halfway, and "E" throwing the hips to the front.

```{r libraries, include=FALSE}
library(caret) #It contains the machine learning functions
library(dplyr) #To modify data frames
library(parallel) #For parallel computations
library(doParallel) #For parallel computations
```

```{r downloading files, cache=TRUE}
#I download and store the training data set
if(!file.exists("pml-training.csv")) {
  download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",destfile="pml-training.csv")
}
training <- read.csv("pml-training.csv")

#I download and store the testing data set
if(!file.exists("pml-testing.csv")) {
  download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",destfile="pml-testing.csv")
}
testing <- read.csv("pml-testing.csv")

```

We download from https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv and "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv the training and testing sets, respectively. As usual with machine learning algorithms, we just analyze the training set, find a model to describe the outcome ("Classe"), and then apply the model to the testing frame.

The text is organized as follows. In the next section, we clean the data frames (training and testing). We then build a machine-learning model, in particular a random forest, with our training set. We finally apply the model to our testing set. We leave the R code for an appendix.

## Cleaning the data frames

Exploring the training data frame, we decide to get rid of the following variables to speed up the simulations:

1) The number of observation "X" (it is irrelevant for any model).

2) Those variables whose entries are mostly NA values (we do not specify here which ones are those, since we detect them automatically in the code; see appendix for more details).

3) Variables whose entries are mostly blank (but no NA): kurtosis_roll_belt, kurtosis_picth_belt, kurtosis_yaw_belt, skewness_roll_belt, skewness_roll_belt.1, skewness_yaw_belt, max_yaw_belt, min_yaw_belt, amplitude_yaw_belt, kurtosis_roll_arm, kurtosis_picth_arm, kurtosis_yaw_arm, skewness_roll_arm, skewness_pitch_arm, skewness_yaw_arm, kurtosis_roll_dumbbell, kurtosis_picth_dumbbell, kurtosis_yaw_dumbbell, skewness_roll_dumbbell, skewness_pitch_dumbbell, skewness_yaw_dumbbell, max_yaw_dumbbell, min_yaw_dumbbell, amplitude_yaw_dumbbell, kurtosis_roll_forearm, kurtosis_picth_forearm, kurtosis_yaw_forearm, skewness_roll_forearm, skewness_pitch_forearm, skewness_yaw_forearm, max_yaw_forearm, min_yaw_forearm, and amplitude_yaw_forearm.

4) Finally, we get rid of cvtd_timestamp, since the moment in which the activity was performed should not have an impact on how well it was done, and new_window, since that variable has hardly variability (19216 obesrvations correspond to "no", whereas just 406 are "yes").

Notice that we apply the very same transformations to the testing set.

```{r subsettion the non-NA variables}
#As one can see quickly, those variables with some NA values are such that **MOST** of the entries are actually NA. In the following, we subset the data frames training/testing, excluding the variables with NA values.
x <- lapply(training,function(y){sum(is.na(y))})
x2 <- x==0
training <- training[,x2]
testing <- testing[,x2]

#We exclude here the variables such that most of their values are blank, as well as the number of observation "X". We apply the same transformation to both data frames
training <- select(training,-c(X,kurtosis_roll_belt, kurtosis_picth_belt, kurtosis_yaw_belt, skewness_roll_belt, skewness_roll_belt.1, skewness_yaw_belt, max_yaw_belt, min_yaw_belt, amplitude_yaw_belt, kurtosis_roll_arm, kurtosis_picth_arm, kurtosis_yaw_arm, skewness_roll_arm, skewness_pitch_arm, skewness_yaw_arm, kurtosis_roll_dumbbell, kurtosis_picth_dumbbell, kurtosis_yaw_dumbbell, skewness_roll_dumbbell, skewness_pitch_dumbbell, skewness_yaw_dumbbell, max_yaw_dumbbell, min_yaw_dumbbell, amplitude_yaw_dumbbell, kurtosis_roll_forearm, kurtosis_picth_forearm, kurtosis_yaw_forearm, skewness_roll_forearm, skewness_pitch_forearm, skewness_yaw_forearm, max_yaw_forearm, min_yaw_forearm, amplitude_yaw_forearm))

testing <- select(testing,-c(X,kurtosis_roll_belt, kurtosis_picth_belt, kurtosis_yaw_belt, skewness_roll_belt, skewness_roll_belt.1, skewness_yaw_belt, max_yaw_belt, min_yaw_belt, amplitude_yaw_belt, kurtosis_roll_arm, kurtosis_picth_arm, kurtosis_yaw_arm, skewness_roll_arm, skewness_pitch_arm, skewness_yaw_arm, kurtosis_roll_dumbbell, kurtosis_picth_dumbbell, kurtosis_yaw_dumbbell, skewness_roll_dumbbell, skewness_pitch_dumbbell, skewness_yaw_dumbbell, max_yaw_dumbbell, min_yaw_dumbbell, amplitude_yaw_dumbbell, kurtosis_roll_forearm, kurtosis_picth_forearm, kurtosis_yaw_forearm, skewness_roll_forearm, skewness_pitch_forearm, skewness_yaw_forearm, max_yaw_forearm, min_yaw_forearm, amplitude_yaw_forearm))

#We finally get rid of the variables cvtd_timestamp and new_window
training <- select(training,-c(cvtd_timestamp,new_window))
testing <- select(testing,-c(cvtd_timestamp,new_window))
```

## Machine learning algorithm

### Generating new training and testing sets

Even if we already have a training and a testing data set, there are two reasons to split once again our training set into two new training and testing data sets:

1) We do not know the values of "classe" for the testing data set, so we cannot make sure whether our model works or not with those data and

2) the training set is large enough (it has 19622 observations) to split it.

We then split the training set into two new training and testing sets, with 70% and 30% of the data in each new frame respectively using the function "createDataPartition", from caret package.

```{r partition}
#We initialize the seed in order to make the report reproducible
set.seed(112)
#We generate the partition
inTrain <- createDataPartition(y=training$classe,p=.7,list=F)
#Defining the new training set
training_training <- training[inTrain,]
#Defining the new testing set
training_testing <- training[-inTrain,]
```

### Building the model

To start with, we use all the variables from the data frame as predictors but those we excluded in the previous section. As we will see, these variables give an accurate description of the problem.

We then build a machine-learning model with the new training set. We decide to use random forest. In order to cross-validate the model, we use 5-fold cross validation. Once we build the model, we apply it to the new testing set. We compare the predictions to the actual values, showing both the confusion matrix and the accuracy of the predictions:

```{r model, cache=TRUE}
#These commands configure the parallel processing
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)

#Specify the trainControl options
fitControl <- trainControl(method="cv",number=5,allowParallel = T)
#Define separately the outcome (Classe) and the predictors (all the other variables)
x <- training_training[,-57]
y <- training_training[,57]
model <- train(x,y,data=training_training,method="rf",trControl=fitControl)

#Shutting down the parallel computation
stopCluster(cluster)
registerDoSEQ()

predictions <- predict(model,newdata=training_testing)
m <- confusionMatrix(predictions,training_testing$classe)
sprintf("The accuracy of the predictions is %f", m$overall[1])
sprintf("We also show the table comparing the predictions to the actual values:")
m$table
```

Random forest describes very well how the physical activity was performed, as seen in the high accuracy and the table comparing the predictions and the actual values of the new testing set. In conclussion, if we apply this model to another data set with the same variables, we expect to get a really small error in our predictions.

## Predicting 

We finally apply the model to the initial testing set:

```{r predict testing}
#We predict the values of Classe for the testing set
predictions_t <- predict(model,newdata=testing)
df_predictions_t <- data.frame(Classe=predictions_t)
sprintf("Table with the predictions corresponding to each observation of the testing frame:")
df_predictions_t
```

Due to the high accuracy obtained in the previous section when building the model, we expect that most, if not all the predictions, are correct.

## Apendix

```{r, ref.label=knitr::all_labels(),echo=TRUE,eval=FALSE}
```